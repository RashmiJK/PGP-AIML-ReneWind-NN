{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RashmiJK/PGP-AIML-ReneWind-NN/blob/main/renewind_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DoY1AHZfMF1"
      },
      "source": [
        "Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n",
        "\n",
        "Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n",
        "\n",
        "Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n",
        "\n",
        "The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p9zp6bHfPOY"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu7YIw90fRdW"
      },
      "source": [
        "“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.\n",
        "\n",
        "The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost.\n",
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n",
        "- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n",
        "- False positives (FP) are detections where there is no failure. These will result in inspection costs.\n",
        "\n",
        "It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\n",
        "\n",
        "“1” in the target variables should be considered as “failure” and “0” represents “No failure”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPeJAS5bfUOj"
      },
      "source": [
        "## Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ns9KiyfVsW"
      },
      "source": [
        "The data provided is a transformed version of the original data which was collected using sensors.\n",
        "\n",
        "- Train.csv - To be used for training and tuning of models.\n",
        "- Test.csv - To be used only for testing the performance of the final best model.\n",
        "\n",
        "Both the datasets consist of 40 predictor variables and 1 target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-uuGqH-qTt"
      },
      "source": [
        "# **1 - Installing and Importing the necessary libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to build classification neural netwrok model to predict the turbine failures.\n",
        "\n",
        "**Instruction:** Restart the runtime after installing libraries to ensure correct package versions and ignore dependency warnings."
      ],
      "metadata": {
        "id": "K-JVICvpSltc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ob7CHevooE2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8edc76b8-075a-41c7-9c21-d1ef40e22bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing the libraries with the specified version\n",
        "!pip install tensorflow==2.18.0 scikit-learn==1.3.2 matplotlib===3.8.3 seaborn==0.13.2 numpy==1.26.4 pandas==2.2.2 -q --user --no-warn-script-location --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbiVVX6hbp0v"
      },
      "outputs": [],
      "source": [
        "# Libraries for data manipulation, analysis and scientific computing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Libraries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Library for time related functions\n",
        "import time\n",
        "\n",
        "# For splitting datasets into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Tools for data preprocessing including label encoding, one-hot encoding, and standard scaling\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder,StandardScaler\n",
        "# Imports a class for imputing missing values in datasets.\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Imports for evaluating the performance of machine learning models\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Imports the tensorflow, keras and layers.\n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,BatchNormalization\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "# To suppress unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "## 2 - Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D4QgYmaOkMEA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "99a49cbe-c357-4773-eb17-44566445dc89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2408960717.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# uncomment and run the following lines for Google Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "# uncomment and run the following lines for Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39UgpBY3bp0y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"_______\")\n",
        "df_test = pd.read_csv(\"_______\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cVx5kZHbp02"
      },
      "source": [
        "# **Data Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-5jUOgu-qTz"
      },
      "source": [
        "The initial steps to get an overview of any dataset is to:\n",
        "- observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- get information about the number of rows and columns in the dataset\n",
        "- find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
        "- check the statistical summary of the dataset to get an overview of the numerical columns of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQi5ygTC-qT1"
      },
      "source": [
        "## Checking the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8epHpjbp0z"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the train data\n",
        "df.______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA15qTjzbp01"
      },
      "outputs": [],
      "source": [
        "# Check the shape of the test data\n",
        "df_test._______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTsYJZBubp02"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the training data\n",
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ImWbLXbp03"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the testing  data\n",
        "data_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlzqMR1K-qTz"
      },
      "source": [
        "## Displaying the first few rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RnN7Twbp03"
      },
      "outputs": [],
      "source": [
        "# View the first 5 rows of the data\n",
        "data.________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47OhpabimjMy"
      },
      "outputs": [],
      "source": [
        "# View the first 5 rows of the test data\n",
        "data_test.______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TcqcxbK-qT3"
      },
      "source": [
        "## Checking the data types of the columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBYJoKkbp04"
      },
      "outputs": [],
      "source": [
        "# View the data types of the columns in the train data\n",
        "data.________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvN3_-Vdbp04"
      },
      "source": [
        "- Converting Target column to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LwVuEgjmHxW"
      },
      "outputs": [],
      "source": [
        "data['Target'] = data['Target'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MmheoOtnYZx"
      },
      "source": [
        "Now checking for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utfm_wZsnR1s"
      },
      "outputs": [],
      "source": [
        "# View the data types of the columns in the test data\n",
        "data_test.________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX9vFVFnk5t"
      },
      "source": [
        "Converting Target to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfTJuArBnbIU"
      },
      "outputs": [],
      "source": [
        "data_test['Target'] = data_test['Target'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNr4bWoM-qT5"
      },
      "source": [
        "## Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0EmBHNmbp04"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates in the train data\n",
        "data.________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch_TjRfF-qT5"
      },
      "source": [
        "## Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwFZm-Jbp05"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in the data\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bar6robp06"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in the test data\n",
        "data_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUCorhch-qT4"
      },
      "source": [
        "## Statistical summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lzvHKCbp06"
      },
      "outputs": [],
      "source": [
        "# View the statistical summary of the numerical columns in the train data\n",
        "data.________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPuzWO7hmV8"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv7Bs8aUbp07"
      },
      "source": [
        "## Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP4bI3Zbp07"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data= ______, x= _______, ax= _______, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=______, x=_______, kde=kde, ax=_______, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=________, x=________, kde=kde, ax=______\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5N52iqsJBeb"
      },
      "source": [
        "### Variables V1 to V29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ip-InCbp07",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    histogram_boxplot(df, feature, figsize=(12, 7), kde=False, bins=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjaGOdvbp08"
      },
      "source": [
        "### Checking the distrubution of Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jchAb6Ikbp08"
      },
      "outputs": [],
      "source": [
        "# For train data\n",
        "df[\"Target\"].value_counts(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rupsM4sbp08"
      },
      "outputs": [],
      "source": [
        "# display the proportion of the target variable in the test data\n",
        "df_test[\"Target\"]._____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRrYccqLJKXi"
      },
      "source": [
        "## Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk0t8FYwJMeL"
      },
      "source": [
        "### Correlation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL32Bf4rJO1F"
      },
      "outputs": [],
      "source": [
        "cols_list = df.select_dtypes(include=np.number).columns.tolist()\n",
        "cols_list.remove(\"Target\")\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(\n",
        "    df[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp8vC9MZbp09"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMTLLop3yDwk"
      },
      "source": [
        "## Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-xgnTbGb4CY"
      },
      "outputs": [],
      "source": [
        "# Dividing train data into X and y\n",
        "\n",
        "# Remove the column named 'Target'\n",
        "X = data.drop(columns = [\"_______\"] , axis=1)\n",
        "\n",
        "# Column named 'Target' becomes y\n",
        "y = data[\"________\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXiW4s4kd63"
      },
      "source": [
        "**Test set is provided separately**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-8vqBMXbp09"
      },
      "outputs": [],
      "source": [
        "# Splitting data into training and validation set:\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=_____, random_state=1, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIsrQQVqRbnY"
      },
      "outputs": [],
      "source": [
        "# Check the shape of X_train data\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3_X_6ZicNfN"
      },
      "outputs": [],
      "source": [
        "# Check the shape of X_val data\n",
        "X_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IktO9G2Lbp09"
      },
      "outputs": [],
      "source": [
        "# Divide test data into X_test and y_test\n",
        "\n",
        "# Drop target column for X\n",
        "X_test = data_test.drop(columns = ['_______'] , axis= 1)\n",
        "\n",
        "# Retain only target column for y\n",
        "y_test = data_test[\"______\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmuB5K_9b8tP"
      },
      "outputs": [],
      "source": [
        "# Checkthe shape of X_test data\n",
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRIcs_56ECHk"
      },
      "source": [
        "## Missing Value Imputation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "- There were few missing values in V1 and V2, we will impute them using the median.\n",
        "- And to avoid data leakage we will impute missing values after splitting train data into train and validation sets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28L1vgAwbp09"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNfbw4rJbp09"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the train data\n",
        "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "# Transform the validation data\n",
        "# Code to impute missing values in the validation set while accounting for data leakage\n",
        "X_val = pd.DataFrame(imputer.______(X_val), columns=X_train.columns)\n",
        "\n",
        "# Transform the test data\n",
        "# Code to impute missing values in the test set while accounting for data leakage\n",
        "X_test = pd.DataFrame(imputer.______(X_test), columns=X_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jdLvwTAbp09"
      },
      "outputs": [],
      "source": [
        "# Make sure no column has missing values in train or test sets\n",
        "print(X_train.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_val.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_test.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdAf7_aNRpyP"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.to_numpy()\n",
        "y_val = y_val.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "# **Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czwxDIZNccQL"
      },
      "source": [
        "## Model Evaluation Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "- Write down the metric of choice with rationale here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWoHuUpjbp0_"
      },
      "source": [
        "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFZRy7tSXnp"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpMmFI9USZYW"
      },
      "outputs": [],
      "source": [
        "def plot(history, name):\n",
        "    \"\"\"\n",
        "    Function to plot loss/accuracy\n",
        "\n",
        "    history: an object which stores the metrics and losses.\n",
        "    name: can be one of Loss or Accuracy\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots() #Creating a subplot with figure and axes.\n",
        "    plt.plot(history.history[name]) #Plotting the train accuracy or train loss\n",
        "    plt.plot(history.history['val_'+name]) #Plotting the validation accuracy or validation loss\n",
        "\n",
        "    plt.title('Model ' + name.capitalize()) #Defining the title of the plot.\n",
        "    plt.ylabel(name.capitalize()) #Capitalizing the first letter.\n",
        "    plt.xlabel('Epoch') #Defining the label for the x-axis.\n",
        "    fig.legend(['Train', 'Validation'], loc=\"outside right upper\") #Defining the legend, loc controls the position of the legend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_VAOcNcScFz"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using statsmodels\n",
        "def model_performance_classification(\n",
        "    model, predictors, target, threshold=0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    threshold: threshold for classifying the observation as class 1\n",
        "    \"\"\"\n",
        "\n",
        "    # checking which probabilities are greater than threshold\n",
        "    pred = model.predict(predictors) > threshold\n",
        "    # pred_temp = model.predict(predictors) > threshold\n",
        "    # # rounding off the above values to get classes\n",
        "    # pred = np.round(pred_temp)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred, average='macro')  # to compute Recall\n",
        "    precision = precision_score(target, pred, average='macro')  # to compute Precision\n",
        "    f1 = f1_score(target, pred, average='macro')  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1 Score\": f1,}, index = [0]\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fI98GOV0pTY"
      },
      "source": [
        "## Initial Model Building (Model 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0usUom8nZOYZ"
      },
      "source": [
        "- Let's start with a neural network consisting of\n",
        "  - just one hidden layer of 7 neurons respectively\n",
        "  - activation function of ReLU.\n",
        "  - SGD as the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OTOG3o1bp0_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Define the batch size and # epochs upfront as we'll be using the same values for all models\n",
        "\n",
        "# Number of epochs to be used in all models\n",
        "epochs = ____\n",
        "\n",
        "# Batch size to be used in all models\n",
        "batch_size = _____"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm61fR_mbp0_",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao_7u1sOZlZz"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "model_0 = Sequential()\n",
        "\n",
        "# Define the number of neurons and the activation function\n",
        "model_0.add(Dense( _____ ,activation=\"_______\",input_dim=X_train.shape[1]))\n",
        "\n",
        "# Define the number of neurons in the output layer\n",
        "model_0.add(Dense( _____ ,activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2dEk01hbp1A"
      },
      "outputs": [],
      "source": [
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsOer36qbp1A"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_0.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwUcBORjbp1A",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_0.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwBB3mRhcCN6"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfLx8ubqTe_n"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr6dBVPNUe3Y"
      },
      "outputs": [],
      "source": [
        "model_0_train_perf = model_performance_classification(model_0, X_train, y_train)\n",
        "model_0_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pdgAup2UfAX"
      },
      "outputs": [],
      "source": [
        "model_0_val_perf = model_performance_classification(model_0,X_val,y_val)\n",
        "model_0_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757b8PflQ9T"
      },
      "source": [
        "Let's check the classification reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0UQ_ZW-KCaX"
      },
      "outputs": [],
      "source": [
        "y_train_pred_0 = model_0.predict(X_train)\n",
        "y_val_pred_0 = model_0.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC6fR5ohKFzh"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_0\",end=\"\\n\\n\")\n",
        "cr_train_model_0 = classification_report(y_train,y_train_pred_0>0.5)\n",
        "print(cr_train_model_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foiSZUcHKFp6"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_0\",end=\"\\n\\n\")\n",
        "cr_val_model_0 = classification_report(y_val,y_val_pred_0>0.5)\n",
        "print(cr_val_model_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tlVEbIWKb6T"
      },
      "source": [
        "# **Model Performance Improvement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grQ6pRK1WZtr"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBr3Eo1iWhLs"
      },
      "source": [
        "- Let's try adding another layer to see if we can improve our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E95a3_v6UfE7"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzB8avdtUfH_"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "model_1 = Sequential()\n",
        "\n",
        "# Define the number of neurons and activation function\n",
        "model_1.add(Dense( ________ ,activation=\"________\",input_dim=X_train.shape[1]))\n",
        "\n",
        "# Define the number of neurons and activation function\n",
        "model_1.add(Dense( ________,activation=\"________\"))\n",
        "\n",
        "# Define the number of neurons in the output layer\n",
        "model_1.add(Dense(_______,activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnuMH8oQUfKf"
      },
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfDQJHYnUfM1"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsPSnSmNUfO4"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_1.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX9bBylsUfRN"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKXH3xAYUfTP"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve7QYfyTUfXn"
      },
      "outputs": [],
      "source": [
        "model_1_train_perf = model_performance_classification(model_1,X_train,y_train)\n",
        "model_1_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09KWUhmONOEf"
      },
      "outputs": [],
      "source": [
        "model_1_val_perf = model_performance_classification(model_1,X_val,y_val)\n",
        "model_1_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS8SFCwJUoe6"
      },
      "outputs": [],
      "source": [
        "y_train_pred_1 = model_1.predict(X_train)\n",
        "y_val_pred_1 = model_1.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZngTFbEUskK"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_1\", end=\"\\n\\n\")\n",
        "cr_train_model_1 = classification_report(y_train,y_train_pred_1 > 0.5)\n",
        "print(cr_train_model_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju0Rzn1kUtRi"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_1\", end=\"\\n\\n\")\n",
        "cr_val_model_1 = classification_report(y_val,y_val_pred_1 > 0.5)\n",
        "print(cr_val_model_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQhdyBg0OJWQ"
      },
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUwTGnuGfgRp"
      },
      "source": [
        "To introduce Regularization in our model, let's set the dropout to 50% after adding the first hidden layer. This step will randomly drop 50% of the neurons before proceeding to the next layer, reducing overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7mdqo7xOO_b"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXbGBf1QOO9B"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(________,activation=\"_________\",input_dim=X_train.shape[1]))  # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dropout(____)) # Complete the code to define the dropout rate\n",
        "model_2.add(Dense(_____,activation = \"______\")) # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dense(_____,activation = \"______\")) # Complete the code to define the number of neurons and activation function\n",
        "model_2.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPu7lC9pOO23"
      },
      "outputs": [],
      "source": [
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7-Y_BmYOOxB"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QPtdXotOOu0"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_2.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_VLN5_FOOss"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yat6HgCOOmV"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdmnaDUxbVvF"
      },
      "source": [
        "Lets check the model performance of model_2 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhHFHpRJOOf1"
      },
      "outputs": [],
      "source": [
        "model_2_train_perf = model_performance_classification(model_2,X_train,y_train)\n",
        "model_2_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnMTEzWtOOW0"
      },
      "outputs": [],
      "source": [
        "model_2_val_perf = model_performance_classification(model_2,X_val,y_val)\n",
        "model_2_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI5dqTACWC5N"
      },
      "outputs": [],
      "source": [
        "y_train_pred_2 = model_2.predict(X_train)\n",
        "y_val_pred_2 = model_2.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZOkixQ9bfnI"
      },
      "source": [
        "Lets check the classification report of model_2 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPtFhE5GWAFM"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_2\", end=\"\\n\\n\")\n",
        "cr_train_model_2 = classification_report(y_train,y_train_pred_2 > 0.5)\n",
        "print(cr_train_model_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE-wWumtWEpx"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_2\", end=\"\\n\\n\")\n",
        "cr_val_model_2 = classification_report(y_val , y_val_pred_2 > 0.5)\n",
        "print(cr_val_model_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ryNh6qVVPm"
      },
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swQ14MhSRwCc"
      },
      "source": [
        "As we have are dealing with an imbalance in class distribution, we should also be using class weights to allow the model to give proportionally more importance to the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFEE44vOkYz"
      },
      "outputs": [],
      "source": [
        "# Calculate class weights for imbalanced dataset\n",
        "cw = (y_train.shape[0]) / np.bincount(y_train.astype(int)) # Convert y_train to integers\n",
        "\n",
        "# Create a dictionary mapping class indices to their respective class weights\n",
        "cw_dict = {}\n",
        "for i in range(cw.shape[0]):\n",
        "    cw_dict[i] = cw[i]\n",
        "\n",
        "cw_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hof3fAu9OkWD"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWebuv4mOkTC"
      },
      "outputs": [],
      "source": [
        "model_3 = Sequential()\n",
        "model_3.add(Dense(_____,activation=\"_____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dropout(_____)) # Complete the code to define the dropout rate\n",
        "model_3.add(Dense(_____,activation=\"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dense(_____, activation = \"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_3.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN5XaqlQOkL1"
      },
      "outputs": [],
      "source": [
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbe-Uvf_OkBd"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()   # defining SGD as the optimizer to be used\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGEYYC5pW5iM"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_3.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs,class_weight=cw_dict)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H-rbOvIW5gJ"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWkRSkTGW5dD"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY6ZCgMGcUOb"
      },
      "source": [
        "Lets check the model performance of model_3 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fryTkbgW5XE"
      },
      "outputs": [],
      "source": [
        "model_3_train_perf = model_performance_classification(model_3,X_train,y_train)\n",
        "model_3_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxmCeXa1W5Oo"
      },
      "outputs": [],
      "source": [
        "model_3_val_perf = model_performance_classification(model_3,X_val,y_val)\n",
        "model_3_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BkGKOeCXUsT"
      },
      "outputs": [],
      "source": [
        "y_train_pred_3 = model_3.predict(X_train)\n",
        "y_val_pred_3 = model_3.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMljwQUdcbNm"
      },
      "source": [
        "Lets check the classification report of model_3 on training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUArAjvkXeh9"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_3\", end=\"\\n\\n\")\n",
        "cr_train_model_3 = classification_report(y_train,y_train_pred_3 > 0.5)\n",
        "print(cr_train_model_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp5gmnufXhmy"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_3\", end=\"\\n\\n\")\n",
        "cr_val_model_3 = classification_report(y_val,y_val_pred_3 > 0.5)\n",
        "print(cr_val_model_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPAuEVDzX0qL"
      },
      "source": [
        "## Model 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydPkh3AdgWRU"
      },
      "source": [
        "Since we have used only SGD optimizer till now, let's use another kind of optimizer and observe its impact on the model performmance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Jx5vOFX0qM"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_LP0NHzX0qP"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "model_4 = Sequential()\n",
        "model_4.add(Dense(_____,activation=\"____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_4.add(Dense(_____,activation=\"____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_4.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY_drKH7X0qP"
      },
      "outputs": [],
      "source": [
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbXDHdCaX0qQ"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()    # defining Adam as the optimizer to be used\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_4.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Viy8MTihX0qQ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_4.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC_tPwLlX0qR"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ1TAwlLX0qR"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KuUcLbdKRb"
      },
      "source": [
        "Lets check the model performance ofr model_4 on training and validation data respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OvkmGu6X0qR"
      },
      "outputs": [],
      "source": [
        "model_4_train_perf = model_performance_classification(model_4,X_train,y_train)\n",
        "model_4_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLE1b8CMX0qR"
      },
      "outputs": [],
      "source": [
        "model_4_val_perf = model_performance_classification(model_4,X_val,y_val)\n",
        "model_4_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg1dMCXSX0qS"
      },
      "outputs": [],
      "source": [
        "y_train_pred_4 = model_4.predict(X_train)\n",
        "y_val_pred_4 = model_4.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OH8cvF2dVjv"
      },
      "source": [
        "Lets check the classification report of model_4 on raining and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-wxc08wX0qS"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_4\", end=\"\\n\\n\")\n",
        "cr_train_model_4 = classification_report(y_train,y_train_pred_4 > 0.5)\n",
        "print(cr_train_model_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyKOGZzEX0qT"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_4\", end=\"\\n\\n\")\n",
        "cr_val_model_4 = classification_report(y_val,y_val_pred_4 > 0.5)\n",
        "print(cr_val_model_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2dZjgH0Yxok"
      },
      "source": [
        "## Model 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9H_G0rzdwC9"
      },
      "source": [
        "This time we will add more layers and dropout while using a different optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKvvEkjYxol"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrPwAVJOYxom"
      },
      "outputs": [],
      "source": [
        "#Initializing the neural network\n",
        "from tensorflow.keras.layers import Dropout\n",
        "model_5 = Sequential()\n",
        "model_5.add(Dense(_____,activation=\"_____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dropout(_____)) #Complete the code to define the dropout rate\n",
        "model_5.add(Dense(_____,activation=\"____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dense(_____, activation = \"____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_5.add(Dense(____,activation=\"____\")) # Complete the code to define the number of neurons and activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU1Q8Z8CYxom"
      },
      "outputs": [],
      "source": [
        "model_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MQXRa63Yxom"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()    # defining Adam as the optimizer to be used\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_5.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nfW1HAYYxon"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_5.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs)\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT96bkirYxon"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV6Ve4KuYxon"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL-94xameUkb"
      },
      "source": [
        "Lets check the model performance of model_5 on the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRiDnwPhYxoo"
      },
      "outputs": [],
      "source": [
        "model_5_train_perf = model_performance_classification(model_5,X_train,y_train)\n",
        "model_5_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyz9ix7KYxoo"
      },
      "outputs": [],
      "source": [
        "model_5_val_perf = model_performance_classification(model_5,X_val,y_val)\n",
        "model_5_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0aSAUTlYxop"
      },
      "outputs": [],
      "source": [
        "y_train_pred_5 = model_5.predict(X_train)\n",
        "y_val_pred_5 = model_5.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DKFcJIehp1"
      },
      "source": [
        "Lets check the classification report of model_5 on training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9XpJpekYxop"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_2\", end=\"\\n\\n\")\n",
        "cr_train_model_5 = classification_report(y_train,y_train_pred_5 > 0.5)\n",
        "print(cr_train_model_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RlRQJOuYxop"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_2\", end=\"\\n\\n\")\n",
        "cr_val_model_5 = classification_report(y_val,y_val_pred_5 > 0.5)\n",
        "print(cr_val_model_5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wDd0RIZQyI"
      },
      "source": [
        "## Model 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0IOkrVWYxop"
      },
      "source": [
        "Let's see how does the model performance change when the model gives higher importance to the minority class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LOpw8cNZQyJ"
      },
      "outputs": [],
      "source": [
        "# clears the current Keras session, resetting all layers and models previously created, freeing up memory and resources.\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUew9nFZQyJ"
      },
      "outputs": [],
      "source": [
        "model_6 = Sequential()\n",
        "model_6.add(Dense(____,activation=\"____\",input_dim=X_train.shape[1])) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dropout(____)) # Complete the code to define the dropout rate\n",
        "model_6.add(Dense(_____,activation=\"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(_____, activation = \"_____\")) # Complete the code to define the number of neurons and activation function\n",
        "model_6.add(Dense(_____,activation=\"sigmoid\")) # Complete the code to define the number of neurons in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_sGGGREZQyJ"
      },
      "outputs": [],
      "source": [
        "model_6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaVP4CM9ZQyJ"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['accuracy']) ## Uncomment this line in case the metric of choice is Accuracy\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Precision']) ## Uncomment this line in case the metric of choice is Precision\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['Recall']) ## Uncomment this line in case the metric of choice is Recall\n",
        "# model_6.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['f1_score']) ## Uncomment this line in case the metric of choice is F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhiZR8WMZQyJ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "history = model_3.fit(X_train, y_train, validation_data=(X_val,y_val) , batch_size=batch_size, epochs=epochs,class_weight=_____, ) # Complete the code such that the model is biased towards the minority class\n",
        "end=time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13AF9v2UZQyJ"
      },
      "outputs": [],
      "source": [
        "print(\"Time taken in seconds \",end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48uod9TsZQyK"
      },
      "outputs": [],
      "source": [
        "plot(history,'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhlzTJsngBev"
      },
      "source": [
        "Lets check the model performance of model_6 on training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fiy0gmSUZQyK"
      },
      "outputs": [],
      "source": [
        "model_6_train_perf = model_performance_classification(model_6,X_train,y_train)\n",
        "model_6_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX8Zu4FLZQyK"
      },
      "outputs": [],
      "source": [
        "model_6_val_perf = model_performance_classification(model_6,X_val,y_val)\n",
        "model_6_val_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXZlBIDLZQyK"
      },
      "outputs": [],
      "source": [
        "y_train_pred_6 = model_6.predict(X_train)\n",
        "y_val_pred_6 = model_6.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNsmMIsxgJE_"
      },
      "source": [
        "Lets check the classification report of model_6 on both training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXeX1QHSZQyK"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Train data Model_3\", end=\"\\n\\n\")\n",
        "cr_train_model_6 = classification_report(y_train,y_train_pred_6 > 0.5)\n",
        "print(cr_train_model_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV3-H6ckZQyK"
      },
      "outputs": [],
      "source": [
        "print(\"Classification Report - Validation data Model_3\", end=\"\\n\\n\")\n",
        "cr_val_model_6 = classification_report(y_val,y_val_pred_6 > 0.5)\n",
        "print(cr_val_model_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O07XgkzY_ot"
      },
      "source": [
        "# **Model Performance Comparison and Final Model Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVYagrg9ZQyK"
      },
      "source": [
        "Now, in order to select the final model, we will compare the performances of all the models for the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZUZD83lu4v"
      },
      "source": [
        "**Training Performance Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQB0PVtbh-MA"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        model_0_train_perf.T,\n",
        "        model_1_train_perf.T,\n",
        "        model_2_train_perf.T,\n",
        "        model_3_train_perf.T,\n",
        "        model_4_train_perf.T,\n",
        "        model_5_train_perf.T,\n",
        "        model_6_train_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Model 0\",\n",
        "    \"Model 1\",\n",
        "    \"Model 2\",\n",
        "    \"Model 3\",\n",
        "    \"Model 4\",\n",
        "    \"Model 5\",\n",
        "    \"Model 6\"\n",
        "]\n",
        "print(\"Training set performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqpgdol7l2Um"
      },
      "source": [
        "**Validation Performance Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzWl6tM8ilz9"
      },
      "outputs": [],
      "source": [
        "# Validation performance comparison\n",
        "\n",
        "models_val_comp_df = pd.concat(\n",
        "    [\n",
        "        model_0_val_perf.T,\n",
        "        model_1_val_perf.T,\n",
        "        model_2_val_perf.T,\n",
        "        model_3_val_perf.T,\n",
        "        model_4_val_perf.T,\n",
        "        model_5_val_perf.T,\n",
        "        model_6_val_perf.T\n",
        "\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_val_comp_df.columns = [\n",
        "    \"Model 0\",\n",
        "    \"Model 1\",\n",
        "    \"Model 2\",\n",
        "    \"Model 3\",\n",
        "    \"Model 4\",\n",
        "    \"Model 5\",\n",
        "    \"Model 6\"\n",
        "]\n",
        "print(\"Validation set performance comparison:\")\n",
        "models_val_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q69xcftQuKyg"
      },
      "source": [
        "**Checking the performance of the best model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pfViqPWq_pO"
      },
      "outputs": [],
      "source": [
        "# best_model = model_0 ## Uncomment this line in case the best model is model_0\n",
        "# best_model = model_1 ## Uncomment this line in case the best model is model_1\n",
        "# best_model = model_2 ## Uncomment this line in case the best model is model_2\n",
        "# best_model = model_3 ## Uncomment this line in case the best model is model_3\n",
        "# best_model = model_4 ## Uncomment this line in case the best model is model_4\n",
        "# best_model = model_5 ## Uncomment this line in case the best model is model_5\n",
        "# best_model = model_6 ## Uncomment this line in case the best model is model_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPxuwyh6j227"
      },
      "outputs": [],
      "source": [
        "# Test set performance for the best model\n",
        "best_model_test_perf = model_performance_classification(best_model,X_test,y_test)\n",
        "best_model_test_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ZzDS8Cqe_b"
      },
      "outputs": [],
      "source": [
        "y_test_pred_best = best_model.predict(X_test)\n",
        "\n",
        "cr_test_best_model = classification_report(y_test, y_test_pred_best>0.5) # Check the classification report of best model on test data.\n",
        "print(cr_test_best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdME3J172AN-"
      },
      "source": [
        "# **Actionable Insights and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkJU-1M5uT5b"
      },
      "source": [
        "- Write down actionable insights here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4lLWtluXz7"
      },
      "source": [
        "- Write down business recommendations here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x3-QehJxbp0t",
        "4p9zp6bHfPOY"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}